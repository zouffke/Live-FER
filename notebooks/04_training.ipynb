{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Training the model",
   "id": "69763f25414876ca"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Imports",
   "id": "9f5f49841651a376"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-21T21:10:30.541635Z",
     "start_time": "2024-12-21T21:10:24.876505Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(os.path.abspath('../scripts'))\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import classification_report\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from scripts.FER_CNN import FERCNN"
   ],
   "id": "7b55d35e9fd97b04",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Vars",
   "id": "99ff8fc5f79fdd0f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-21T21:10:30.585318Z",
     "start_time": "2024-12-21T21:10:30.550655Z"
    }
   },
   "cell_type": "code",
   "source": [
    "training_dataset_path = '../.data/output/balanced_train_dataset.pth'\n",
    "test_dataset_path = '../.data/output/test_dataset.pth'\n",
    "log_dir = '../.data/output/logs'\n",
    "model_output_path = '../scripts/model/model.tar'\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "print(f'Using {device} for this notebook')"
   ],
   "id": "7c30a702d8396f97",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for this notebook\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Datasets",
   "id": "e7442d709ca14e02"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-21T21:10:34.579513Z",
     "start_time": "2024-12-21T21:10:30.747178Z"
    }
   },
   "cell_type": "code",
   "source": [
    "training_dataset = torch.load(training_dataset_path)\n",
    "test_dataset = torch.load(test_dataset_path)"
   ],
   "id": "915d6fed4f08e35e",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zouff\\AppData\\Local\\Temp\\ipykernel_20656\\3400320326.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  training_dataset = torch.load(training_dataset_path)\n",
      "C:\\Users\\zouff\\AppData\\Local\\Temp\\ipykernel_20656\\3400320326.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  test_dataset = torch.load(test_dataset_path)\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Preparation\n",
    "Before we can train the model we need to perform some steps"
   ],
   "id": "90574c2cbc154cd5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Model\n",
    "We will create an instance of the FER CNN model"
   ],
   "id": "7e7ff4589a30ba95"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-21T21:10:34.678190Z",
     "start_time": "2024-12-21T21:10:34.584420Z"
    }
   },
   "cell_type": "code",
   "source": "model = FERCNN().to(device)",
   "id": "671938e8ee8eecaa",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Class weights\n",
    "For the loss functions we will need the weights to make the classes more balanced. We will use following formula to calculate the weights: \n",
    "$$weight_i = \\frac{total samples}{num\\ classes \\cdot class\\ count_i}$$"
   ],
   "id": "bfeefc76fdb4fdc1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-21T21:11:03.990914Z",
     "start_time": "2024-12-21T21:10:34.684449Z"
    }
   },
   "cell_type": "code",
   "source": [
    "labels = [label for _, label in training_dataset]\n",
    "class_counts = Counter(labels)\n",
    "total_samples = len(training_dataset)\n",
    "num_classes = len(class_counts)\n",
    "\n",
    "class_weights = torch.tensor(\n",
    "    [total_samples / (num_classes * class_counts[i]) for i in range(num_classes)],\n",
    "    dtype=torch.float32,\n",
    "    device=device\n",
    ")"
   ],
   "id": "172039076a331d37",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-21T21:11:04.129335Z",
     "start_time": "2024-12-21T21:11:04.107988Z"
    }
   },
   "cell_type": "code",
   "source": "class_weights",
   "id": "1769a934ee9a294e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1., 1., 1., 1.], device='cuda:0')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Above we can see the weights we created. We can use these in the loss functions to make the classes more balanced. Because we balanced the dataset before we can see that the weights are all equal to 1. We will still use these weights in our code for if something would change in the future.",
   "id": "8692c9ff21b60276"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Loss functions\n",
    "The loss function will be used in the training loop to determine how far off the model is from the actual values. We will be using CrossEntropyLoss which uses following formula: \n",
    "$$L = - \\sum_i{y_ilog(\\hat{y}_i)}$$\n",
    "Because we are using our own weights this formula becomes:\n",
    "$$L = - \\sum_i{w_i â‹… y_ilog(\\hat{y}_i)}$$\n"
   ],
   "id": "e96f8070ad5ad39e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-21T21:11:04.142633Z",
     "start_time": "2024-12-21T21:11:04.139868Z"
    }
   },
   "cell_type": "code",
   "source": "criterion = nn.CrossEntropyLoss(weight=class_weights)",
   "id": "58cf0730fb7dbcea",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Optimizer",
   "id": "1c3c2e3ff22569c6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-21T21:11:04.169937Z",
     "start_time": "2024-12-21T21:11:04.165663Z"
    }
   },
   "cell_type": "code",
   "source": "optimizer = optim.Adam(model.parameters(), lr=0.001)",
   "id": "1853ee62ca9bc7e",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Dataloader\n",
    "Below we will create the dataloaders for the model to use"
   ],
   "id": "d2c7b7ea61716736"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-21T21:11:04.182269Z",
     "start_time": "2024-12-21T21:11:04.179176Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_loader = DataLoader(\n",
    "    dataset=training_dataset,\n",
    "    batch_size=512,\n",
    "    shuffle=True,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=512,\n",
    "    shuffle=False,\n",
    "    pin_memory=True,\n",
    ")"
   ],
   "id": "81f43f1b71fabcb6",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Training\n",
    "Now that all the preparation steps are completed we can start training the model."
   ],
   "id": "59af40ce28ba85a7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This function trains a deep learning model for a specified number of epochs. First, it ensures that a new directory is created for logging the training run, naming it based on the next available run number. During each epoch, the model is set to training mode, and the loss is computed using the training data. The optimizer updates the model's parameters based on the gradients calculated from the loss. After training, the model is evaluated on the validation set, and both the training and validation losses are logged to TensorBoard for tracking performance across epochs. This process helps monitor overfitting and model convergence.",
   "id": "ac40968e2b19552"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-21T21:11:04.202819Z",
     "start_time": "2024-12-21T21:11:04.196813Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_model(epochs):\n",
    "    global log_dir\n",
    "    if not os.path.exists(log_dir):\n",
    "        os.makedirs(log_dir)\n",
    "    existing_runs = [int(d[3:]) for d in os.listdir(log_dir) if d.startswith(\"run\") and d[3:].isdigit()]\n",
    "    next_run = max(existing_runs) + 1 if existing_runs else 0\n",
    "    log_dir_run = os.path.join(log_dir, f\"run{next_run}\")\n",
    "\n",
    "    writer = SummaryWriter(log_dir_run)\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss, val_loss = 0.0, 0.0\n",
    "\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        running_loss /= len(train_loader)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, labels in test_loader:\n",
    "                images, labels = images.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
    "                preds = model(images)\n",
    "                loss = criterion(preds, labels)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        val_loss /= len(test_loader)\n",
    "\n",
    "        print(\n",
    "            f\"Epoch [{epoch + 1}/{epochs}],\\n\\tTraining loss: {running_loss:.4f}\\n\\tValidation loss: {val_loss:.4f}\")\n",
    "\n",
    "        writer.add_scalar('Training Loss', running_loss, global_step=epoch + 1)\n",
    "        writer.add_scalar('Validation Loss', val_loss, global_step=epoch + 1)\n",
    "\n",
    "    writer.close()\n"
   ],
   "id": "3d4ab0e9aa81ed6b",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-21T23:26:38.581692Z",
     "start_time": "2024-12-21T21:11:04.215042Z"
    }
   },
   "cell_type": "code",
   "source": "train_model(200)",
   "id": "3aa59928aec2e609",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/200],\n",
      "\tTraining loss: 1.9087\n",
      "\tValidation loss: 1.8838\n",
      "Epoch [2/200],\n",
      "\tTraining loss: 1.8343\n",
      "\tValidation loss: 1.8380\n",
      "Epoch [3/200],\n",
      "\tTraining loss: 1.7398\n",
      "\tValidation loss: 1.7061\n",
      "Epoch [4/200],\n",
      "\tTraining loss: 1.6569\n",
      "\tValidation loss: 1.6665\n",
      "Epoch [5/200],\n",
      "\tTraining loss: 1.6028\n",
      "\tValidation loss: 1.5764\n",
      "Epoch [6/200],\n",
      "\tTraining loss: 1.5465\n",
      "\tValidation loss: 1.6214\n",
      "Epoch [7/200],\n",
      "\tTraining loss: 1.5132\n",
      "\tValidation loss: 1.5688\n",
      "Epoch [8/200],\n",
      "\tTraining loss: 1.4866\n",
      "\tValidation loss: 1.5019\n",
      "Epoch [9/200],\n",
      "\tTraining loss: 1.4553\n",
      "\tValidation loss: 1.5051\n",
      "Epoch [10/200],\n",
      "\tTraining loss: 1.4359\n",
      "\tValidation loss: 1.5464\n",
      "Epoch [11/200],\n",
      "\tTraining loss: 1.4157\n",
      "\tValidation loss: 1.4831\n",
      "Epoch [12/200],\n",
      "\tTraining loss: 1.4003\n",
      "\tValidation loss: 1.4618\n",
      "Epoch [13/200],\n",
      "\tTraining loss: 1.3843\n",
      "\tValidation loss: 1.4610\n",
      "Epoch [14/200],\n",
      "\tTraining loss: 1.3678\n",
      "\tValidation loss: 1.4313\n",
      "Epoch [15/200],\n",
      "\tTraining loss: 1.3559\n",
      "\tValidation loss: 1.4147\n",
      "Epoch [16/200],\n",
      "\tTraining loss: 1.3495\n",
      "\tValidation loss: 1.4040\n",
      "Epoch [17/200],\n",
      "\tTraining loss: 1.3351\n",
      "\tValidation loss: 1.3743\n",
      "Epoch [18/200],\n",
      "\tTraining loss: 1.3261\n",
      "\tValidation loss: 1.4170\n",
      "Epoch [19/200],\n",
      "\tTraining loss: 1.3189\n",
      "\tValidation loss: 1.4035\n",
      "Epoch [20/200],\n",
      "\tTraining loss: 1.3112\n",
      "\tValidation loss: 1.4031\n",
      "Epoch [21/200],\n",
      "\tTraining loss: 1.2992\n",
      "\tValidation loss: 1.3967\n",
      "Epoch [22/200],\n",
      "\tTraining loss: 1.2954\n",
      "\tValidation loss: 1.3850\n",
      "Epoch [23/200],\n",
      "\tTraining loss: 1.2850\n",
      "\tValidation loss: 1.4031\n",
      "Epoch [24/200],\n",
      "\tTraining loss: 1.2815\n",
      "\tValidation loss: 1.3782\n",
      "Epoch [25/200],\n",
      "\tTraining loss: 1.2765\n",
      "\tValidation loss: 1.3638\n",
      "Epoch [26/200],\n",
      "\tTraining loss: 1.2683\n",
      "\tValidation loss: 1.3454\n",
      "Epoch [27/200],\n",
      "\tTraining loss: 1.2637\n",
      "\tValidation loss: 1.3210\n",
      "Epoch [28/200],\n",
      "\tTraining loss: 1.2637\n",
      "\tValidation loss: 1.3508\n",
      "Epoch [29/200],\n",
      "\tTraining loss: 1.2516\n",
      "\tValidation loss: 1.3687\n",
      "Epoch [30/200],\n",
      "\tTraining loss: 1.2524\n",
      "\tValidation loss: 1.3446\n",
      "Epoch [31/200],\n",
      "\tTraining loss: 1.2417\n",
      "\tValidation loss: 1.3928\n",
      "Epoch [32/200],\n",
      "\tTraining loss: 1.2427\n",
      "\tValidation loss: 1.3443\n",
      "Epoch [33/200],\n",
      "\tTraining loss: 1.2335\n",
      "\tValidation loss: 1.3361\n",
      "Epoch [34/200],\n",
      "\tTraining loss: 1.2303\n",
      "\tValidation loss: 1.3730\n",
      "Epoch [35/200],\n",
      "\tTraining loss: 1.2263\n",
      "\tValidation loss: 1.3767\n",
      "Epoch [36/200],\n",
      "\tTraining loss: 1.2255\n",
      "\tValidation loss: 1.3130\n",
      "Epoch [37/200],\n",
      "\tTraining loss: 1.2177\n",
      "\tValidation loss: 1.2864\n",
      "Epoch [38/200],\n",
      "\tTraining loss: 1.2102\n",
      "\tValidation loss: 1.3364\n",
      "Epoch [39/200],\n",
      "\tTraining loss: 1.2167\n",
      "\tValidation loss: 1.3502\n",
      "Epoch [40/200],\n",
      "\tTraining loss: 1.2097\n",
      "\tValidation loss: 1.3245\n",
      "Epoch [41/200],\n",
      "\tTraining loss: 1.2033\n",
      "\tValidation loss: 1.3098\n",
      "Epoch [42/200],\n",
      "\tTraining loss: 1.2015\n",
      "\tValidation loss: 1.3397\n",
      "Epoch [43/200],\n",
      "\tTraining loss: 1.1991\n",
      "\tValidation loss: 1.3635\n",
      "Epoch [44/200],\n",
      "\tTraining loss: 1.1985\n",
      "\tValidation loss: 1.3596\n",
      "Epoch [45/200],\n",
      "\tTraining loss: 1.1968\n",
      "\tValidation loss: 1.3312\n",
      "Epoch [46/200],\n",
      "\tTraining loss: 1.1886\n",
      "\tValidation loss: 1.3317\n",
      "Epoch [47/200],\n",
      "\tTraining loss: 1.1855\n",
      "\tValidation loss: 1.2896\n",
      "Epoch [48/200],\n",
      "\tTraining loss: 1.1831\n",
      "\tValidation loss: 1.2916\n",
      "Epoch [49/200],\n",
      "\tTraining loss: 1.1891\n",
      "\tValidation loss: 1.3055\n",
      "Epoch [50/200],\n",
      "\tTraining loss: 1.1814\n",
      "\tValidation loss: 1.3319\n",
      "Epoch [51/200],\n",
      "\tTraining loss: 1.1822\n",
      "\tValidation loss: 1.3079\n",
      "Epoch [52/200],\n",
      "\tTraining loss: 1.1757\n",
      "\tValidation loss: 1.3400\n",
      "Epoch [53/200],\n",
      "\tTraining loss: 1.1779\n",
      "\tValidation loss: 1.3219\n",
      "Epoch [54/200],\n",
      "\tTraining loss: 1.1740\n",
      "\tValidation loss: 1.3175\n",
      "Epoch [55/200],\n",
      "\tTraining loss: 1.1711\n",
      "\tValidation loss: 1.3210\n",
      "Epoch [56/200],\n",
      "\tTraining loss: 1.1702\n",
      "\tValidation loss: 1.2951\n",
      "Epoch [57/200],\n",
      "\tTraining loss: 1.1648\n",
      "\tValidation loss: 1.3125\n",
      "Epoch [58/200],\n",
      "\tTraining loss: 1.1596\n",
      "\tValidation loss: 1.3142\n",
      "Epoch [59/200],\n",
      "\tTraining loss: 1.1708\n",
      "\tValidation loss: 1.3229\n",
      "Epoch [60/200],\n",
      "\tTraining loss: 1.1609\n",
      "\tValidation loss: 1.3319\n",
      "Epoch [61/200],\n",
      "\tTraining loss: 1.1590\n",
      "\tValidation loss: 1.3472\n",
      "Epoch [62/200],\n",
      "\tTraining loss: 1.1592\n",
      "\tValidation loss: 1.3196\n",
      "Epoch [63/200],\n",
      "\tTraining loss: 1.1556\n",
      "\tValidation loss: 1.2942\n",
      "Epoch [64/200],\n",
      "\tTraining loss: 1.1538\n",
      "\tValidation loss: 1.2759\n",
      "Epoch [65/200],\n",
      "\tTraining loss: 1.1538\n",
      "\tValidation loss: 1.2829\n",
      "Epoch [66/200],\n",
      "\tTraining loss: 1.1550\n",
      "\tValidation loss: 1.2790\n",
      "Epoch [67/200],\n",
      "\tTraining loss: 1.1509\n",
      "\tValidation loss: 1.2784\n",
      "Epoch [68/200],\n",
      "\tTraining loss: 1.1443\n",
      "\tValidation loss: 1.3112\n",
      "Epoch [69/200],\n",
      "\tTraining loss: 1.1453\n",
      "\tValidation loss: 1.3043\n",
      "Epoch [70/200],\n",
      "\tTraining loss: 1.1446\n",
      "\tValidation loss: 1.2869\n",
      "Epoch [71/200],\n",
      "\tTraining loss: 1.1428\n",
      "\tValidation loss: 1.3101\n",
      "Epoch [72/200],\n",
      "\tTraining loss: 1.1433\n",
      "\tValidation loss: 1.3024\n",
      "Epoch [73/200],\n",
      "\tTraining loss: 1.1416\n",
      "\tValidation loss: 1.2839\n",
      "Epoch [74/200],\n",
      "\tTraining loss: 1.1361\n",
      "\tValidation loss: 1.2946\n",
      "Epoch [75/200],\n",
      "\tTraining loss: 1.1397\n",
      "\tValidation loss: 1.2503\n",
      "Epoch [76/200],\n",
      "\tTraining loss: 1.1398\n",
      "\tValidation loss: 1.2908\n",
      "Epoch [77/200],\n",
      "\tTraining loss: 1.1405\n",
      "\tValidation loss: 1.2961\n",
      "Epoch [78/200],\n",
      "\tTraining loss: 1.1356\n",
      "\tValidation loss: 1.3252\n",
      "Epoch [79/200],\n",
      "\tTraining loss: 1.1364\n",
      "\tValidation loss: 1.2594\n",
      "Epoch [80/200],\n",
      "\tTraining loss: 1.1249\n",
      "\tValidation loss: 1.3114\n",
      "Epoch [81/200],\n",
      "\tTraining loss: 1.1305\n",
      "\tValidation loss: 1.2840\n",
      "Epoch [82/200],\n",
      "\tTraining loss: 1.1261\n",
      "\tValidation loss: 1.3245\n",
      "Epoch [83/200],\n",
      "\tTraining loss: 1.1270\n",
      "\tValidation loss: 1.2488\n",
      "Epoch [84/200],\n",
      "\tTraining loss: 1.1197\n",
      "\tValidation loss: 1.2905\n",
      "Epoch [85/200],\n",
      "\tTraining loss: 1.1228\n",
      "\tValidation loss: 1.2989\n",
      "Epoch [86/200],\n",
      "\tTraining loss: 1.1268\n",
      "\tValidation loss: 1.2551\n",
      "Epoch [87/200],\n",
      "\tTraining loss: 1.1218\n",
      "\tValidation loss: 1.2517\n",
      "Epoch [88/200],\n",
      "\tTraining loss: 1.1185\n",
      "\tValidation loss: 1.2492\n",
      "Epoch [89/200],\n",
      "\tTraining loss: 1.1227\n",
      "\tValidation loss: 1.2737\n",
      "Epoch [90/200],\n",
      "\tTraining loss: 1.1244\n",
      "\tValidation loss: 1.2851\n",
      "Epoch [91/200],\n",
      "\tTraining loss: 1.1271\n",
      "\tValidation loss: 1.3334\n",
      "Epoch [92/200],\n",
      "\tTraining loss: 1.1187\n",
      "\tValidation loss: 1.2711\n",
      "Epoch [93/200],\n",
      "\tTraining loss: 1.1171\n",
      "\tValidation loss: 1.2832\n",
      "Epoch [94/200],\n",
      "\tTraining loss: 1.1173\n",
      "\tValidation loss: 1.3137\n",
      "Epoch [95/200],\n",
      "\tTraining loss: 1.1099\n",
      "\tValidation loss: 1.2825\n",
      "Epoch [96/200],\n",
      "\tTraining loss: 1.1105\n",
      "\tValidation loss: 1.2705\n",
      "Epoch [97/200],\n",
      "\tTraining loss: 1.1118\n",
      "\tValidation loss: 1.3049\n",
      "Epoch [98/200],\n",
      "\tTraining loss: 1.1096\n",
      "\tValidation loss: 1.2659\n",
      "Epoch [99/200],\n",
      "\tTraining loss: 1.1083\n",
      "\tValidation loss: 1.2609\n",
      "Epoch [100/200],\n",
      "\tTraining loss: 1.1074\n",
      "\tValidation loss: 1.2845\n",
      "Epoch [101/200],\n",
      "\tTraining loss: 1.1067\n",
      "\tValidation loss: 1.2524\n",
      "Epoch [102/200],\n",
      "\tTraining loss: 1.1082\n",
      "\tValidation loss: 1.2886\n",
      "Epoch [103/200],\n",
      "\tTraining loss: 1.1071\n",
      "\tValidation loss: 1.2719\n",
      "Epoch [104/200],\n",
      "\tTraining loss: 1.1029\n",
      "\tValidation loss: 1.2825\n",
      "Epoch [105/200],\n",
      "\tTraining loss: 1.1081\n",
      "\tValidation loss: 1.2807\n",
      "Epoch [106/200],\n",
      "\tTraining loss: 1.1019\n",
      "\tValidation loss: 1.2943\n",
      "Epoch [107/200],\n",
      "\tTraining loss: 1.1035\n",
      "\tValidation loss: 1.3020\n",
      "Epoch [108/200],\n",
      "\tTraining loss: 1.1004\n",
      "\tValidation loss: 1.2739\n",
      "Epoch [109/200],\n",
      "\tTraining loss: 1.0943\n",
      "\tValidation loss: 1.2627\n",
      "Epoch [110/200],\n",
      "\tTraining loss: 1.0982\n",
      "\tValidation loss: 1.2690\n",
      "Epoch [111/200],\n",
      "\tTraining loss: 1.1049\n",
      "\tValidation loss: 1.2692\n",
      "Epoch [112/200],\n",
      "\tTraining loss: 1.0994\n",
      "\tValidation loss: 1.2840\n",
      "Epoch [113/200],\n",
      "\tTraining loss: 1.0912\n",
      "\tValidation loss: 1.2787\n",
      "Epoch [114/200],\n",
      "\tTraining loss: 1.0974\n",
      "\tValidation loss: 1.2492\n",
      "Epoch [115/200],\n",
      "\tTraining loss: 1.0983\n",
      "\tValidation loss: 1.3110\n",
      "Epoch [116/200],\n",
      "\tTraining loss: 1.0962\n",
      "\tValidation loss: 1.2747\n",
      "Epoch [117/200],\n",
      "\tTraining loss: 1.0902\n",
      "\tValidation loss: 1.2472\n",
      "Epoch [118/200],\n",
      "\tTraining loss: 1.0842\n",
      "\tValidation loss: 1.2810\n",
      "Epoch [119/200],\n",
      "\tTraining loss: 1.0924\n",
      "\tValidation loss: 1.2831\n",
      "Epoch [120/200],\n",
      "\tTraining loss: 1.0885\n",
      "\tValidation loss: 1.3022\n",
      "Epoch [121/200],\n",
      "\tTraining loss: 1.0969\n",
      "\tValidation loss: 1.2558\n",
      "Epoch [122/200],\n",
      "\tTraining loss: 1.0845\n",
      "\tValidation loss: 1.2492\n",
      "Epoch [123/200],\n",
      "\tTraining loss: 1.0904\n",
      "\tValidation loss: 1.2566\n",
      "Epoch [124/200],\n",
      "\tTraining loss: 1.0869\n",
      "\tValidation loss: 1.2431\n",
      "Epoch [125/200],\n",
      "\tTraining loss: 1.0843\n",
      "\tValidation loss: 1.2567\n",
      "Epoch [126/200],\n",
      "\tTraining loss: 1.0898\n",
      "\tValidation loss: 1.3026\n",
      "Epoch [127/200],\n",
      "\tTraining loss: 1.0837\n",
      "\tValidation loss: 1.2928\n",
      "Epoch [128/200],\n",
      "\tTraining loss: 1.0798\n",
      "\tValidation loss: 1.2670\n",
      "Epoch [129/200],\n",
      "\tTraining loss: 1.0866\n",
      "\tValidation loss: 1.2338\n",
      "Epoch [130/200],\n",
      "\tTraining loss: 1.0829\n",
      "\tValidation loss: 1.2588\n",
      "Epoch [131/200],\n",
      "\tTraining loss: 1.0830\n",
      "\tValidation loss: 1.2476\n",
      "Epoch [132/200],\n",
      "\tTraining loss: 1.0857\n",
      "\tValidation loss: 1.2768\n",
      "Epoch [133/200],\n",
      "\tTraining loss: 1.0829\n",
      "\tValidation loss: 1.2498\n",
      "Epoch [134/200],\n",
      "\tTraining loss: 1.0821\n",
      "\tValidation loss: 1.2846\n",
      "Epoch [135/200],\n",
      "\tTraining loss: 1.0811\n",
      "\tValidation loss: 1.2736\n",
      "Epoch [136/200],\n",
      "\tTraining loss: 1.0741\n",
      "\tValidation loss: 1.2933\n",
      "Epoch [137/200],\n",
      "\tTraining loss: 1.0775\n",
      "\tValidation loss: 1.2919\n",
      "Epoch [138/200],\n",
      "\tTraining loss: 1.0777\n",
      "\tValidation loss: 1.2596\n",
      "Epoch [139/200],\n",
      "\tTraining loss: 1.0735\n",
      "\tValidation loss: 1.2578\n",
      "Epoch [140/200],\n",
      "\tTraining loss: 1.0811\n",
      "\tValidation loss: 1.2523\n",
      "Epoch [141/200],\n",
      "\tTraining loss: 1.0744\n",
      "\tValidation loss: 1.2899\n",
      "Epoch [142/200],\n",
      "\tTraining loss: 1.0763\n",
      "\tValidation loss: 1.2532\n",
      "Epoch [143/200],\n",
      "\tTraining loss: 1.0732\n",
      "\tValidation loss: 1.2526\n",
      "Epoch [144/200],\n",
      "\tTraining loss: 1.0803\n",
      "\tValidation loss: 1.2471\n",
      "Epoch [145/200],\n",
      "\tTraining loss: 1.0737\n",
      "\tValidation loss: 1.2370\n",
      "Epoch [146/200],\n",
      "\tTraining loss: 1.0740\n",
      "\tValidation loss: 1.2710\n",
      "Epoch [147/200],\n",
      "\tTraining loss: 1.0794\n",
      "\tValidation loss: 1.2799\n",
      "Epoch [148/200],\n",
      "\tTraining loss: 1.0723\n",
      "\tValidation loss: 1.2808\n",
      "Epoch [149/200],\n",
      "\tTraining loss: 1.0750\n",
      "\tValidation loss: 1.2835\n",
      "Epoch [150/200],\n",
      "\tTraining loss: 1.0690\n",
      "\tValidation loss: 1.2325\n",
      "Epoch [151/200],\n",
      "\tTraining loss: 1.0700\n",
      "\tValidation loss: 1.2695\n",
      "Epoch [152/200],\n",
      "\tTraining loss: 1.0688\n",
      "\tValidation loss: 1.2607\n",
      "Epoch [153/200],\n",
      "\tTraining loss: 1.0684\n",
      "\tValidation loss: 1.2730\n",
      "Epoch [154/200],\n",
      "\tTraining loss: 1.0759\n",
      "\tValidation loss: 1.2664\n",
      "Epoch [155/200],\n",
      "\tTraining loss: 1.0660\n",
      "\tValidation loss: 1.2596\n",
      "Epoch [156/200],\n",
      "\tTraining loss: 1.0673\n",
      "\tValidation loss: 1.2915\n",
      "Epoch [157/200],\n",
      "\tTraining loss: 1.0698\n",
      "\tValidation loss: 1.2348\n",
      "Epoch [158/200],\n",
      "\tTraining loss: 1.0645\n",
      "\tValidation loss: 1.2844\n",
      "Epoch [159/200],\n",
      "\tTraining loss: 1.0693\n",
      "\tValidation loss: 1.2615\n",
      "Epoch [160/200],\n",
      "\tTraining loss: 1.0621\n",
      "\tValidation loss: 1.2288\n",
      "Epoch [161/200],\n",
      "\tTraining loss: 1.0595\n",
      "\tValidation loss: 1.2833\n",
      "Epoch [162/200],\n",
      "\tTraining loss: 1.0595\n",
      "\tValidation loss: 1.2513\n",
      "Epoch [163/200],\n",
      "\tTraining loss: 1.0648\n",
      "\tValidation loss: 1.2761\n",
      "Epoch [164/200],\n",
      "\tTraining loss: 1.0645\n",
      "\tValidation loss: 1.2564\n",
      "Epoch [165/200],\n",
      "\tTraining loss: 1.0591\n",
      "\tValidation loss: 1.2679\n",
      "Epoch [166/200],\n",
      "\tTraining loss: 1.0623\n",
      "\tValidation loss: 1.2650\n",
      "Epoch [167/200],\n",
      "\tTraining loss: 1.0609\n",
      "\tValidation loss: 1.2107\n",
      "Epoch [168/200],\n",
      "\tTraining loss: 1.0569\n",
      "\tValidation loss: 1.2514\n",
      "Epoch [169/200],\n",
      "\tTraining loss: 1.0629\n",
      "\tValidation loss: 1.2568\n",
      "Epoch [170/200],\n",
      "\tTraining loss: 1.0625\n",
      "\tValidation loss: 1.2797\n",
      "Epoch [171/200],\n",
      "\tTraining loss: 1.0568\n",
      "\tValidation loss: 1.2452\n",
      "Epoch [172/200],\n",
      "\tTraining loss: 1.0557\n",
      "\tValidation loss: 1.2571\n",
      "Epoch [173/200],\n",
      "\tTraining loss: 1.0566\n",
      "\tValidation loss: 1.2588\n",
      "Epoch [174/200],\n",
      "\tTraining loss: 1.0570\n",
      "\tValidation loss: 1.2667\n",
      "Epoch [175/200],\n",
      "\tTraining loss: 1.0562\n",
      "\tValidation loss: 1.2504\n",
      "Epoch [176/200],\n",
      "\tTraining loss: 1.0513\n",
      "\tValidation loss: 1.2729\n",
      "Epoch [177/200],\n",
      "\tTraining loss: 1.0607\n",
      "\tValidation loss: 1.2445\n",
      "Epoch [178/200],\n",
      "\tTraining loss: 1.0587\n",
      "\tValidation loss: 1.2761\n",
      "Epoch [179/200],\n",
      "\tTraining loss: 1.0535\n",
      "\tValidation loss: 1.2411\n",
      "Epoch [180/200],\n",
      "\tTraining loss: 1.0505\n",
      "\tValidation loss: 1.2444\n",
      "Epoch [181/200],\n",
      "\tTraining loss: 1.0603\n",
      "\tValidation loss: 1.2342\n",
      "Epoch [182/200],\n",
      "\tTraining loss: 1.0545\n",
      "\tValidation loss: 1.2510\n",
      "Epoch [183/200],\n",
      "\tTraining loss: 1.0496\n",
      "\tValidation loss: 1.2285\n",
      "Epoch [184/200],\n",
      "\tTraining loss: 1.0515\n",
      "\tValidation loss: 1.2534\n",
      "Epoch [185/200],\n",
      "\tTraining loss: 1.0498\n",
      "\tValidation loss: 1.2705\n",
      "Epoch [186/200],\n",
      "\tTraining loss: 1.0545\n",
      "\tValidation loss: 1.2735\n",
      "Epoch [187/200],\n",
      "\tTraining loss: 1.0524\n",
      "\tValidation loss: 1.2458\n",
      "Epoch [188/200],\n",
      "\tTraining loss: 1.0525\n",
      "\tValidation loss: 1.2576\n",
      "Epoch [189/200],\n",
      "\tTraining loss: 1.0558\n",
      "\tValidation loss: 1.2526\n",
      "Epoch [190/200],\n",
      "\tTraining loss: 1.0505\n",
      "\tValidation loss: 1.2325\n",
      "Epoch [191/200],\n",
      "\tTraining loss: 1.0492\n",
      "\tValidation loss: 1.2810\n",
      "Epoch [192/200],\n",
      "\tTraining loss: 1.0521\n",
      "\tValidation loss: 1.2795\n",
      "Epoch [193/200],\n",
      "\tTraining loss: 1.0495\n",
      "\tValidation loss: 1.2423\n",
      "Epoch [194/200],\n",
      "\tTraining loss: 1.0477\n",
      "\tValidation loss: 1.2529\n",
      "Epoch [195/200],\n",
      "\tTraining loss: 1.0527\n",
      "\tValidation loss: 1.2539\n",
      "Epoch [196/200],\n",
      "\tTraining loss: 1.0435\n",
      "\tValidation loss: 1.2664\n",
      "Epoch [197/200],\n",
      "\tTraining loss: 1.0492\n",
      "\tValidation loss: 1.2755\n",
      "Epoch [198/200],\n",
      "\tTraining loss: 1.0428\n",
      "\tValidation loss: 1.2160\n",
      "Epoch [199/200],\n",
      "\tTraining loss: 1.0454\n",
      "\tValidation loss: 1.2930\n",
      "Epoch [200/200],\n",
      "\tTraining loss: 1.0424\n",
      "\tValidation loss: 1.2417\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Evaluating the model",
   "id": "96d854e3d68c3040"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-21T23:26:38.604397Z",
     "start_time": "2024-12-21T23:26:38.598824Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def evaluate():\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    emotions = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    print(f\"Test Accuracy: {(correct / total) * 100:.2f}%\")\n",
    "    print(classification_report(all_labels, all_preds, target_names=emotions))"
   ],
   "id": "31b9e4f5cd386837",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-22T09:02:09.638041Z",
     "start_time": "2024-12-22T09:02:04.878132Z"
    }
   },
   "cell_type": "code",
   "source": "evaluate()",
   "id": "5634c941373aa11c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 56.44%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Angry       0.47      0.47      0.47       958\n",
      "     Disgust       0.28      0.77      0.41       111\n",
      "        Fear       0.42      0.25      0.31      1024\n",
      "       Happy       0.81      0.77      0.79      1774\n",
      "         Sad       0.42      0.54      0.47      1247\n",
      "    Surprise       0.70      0.73      0.71       831\n",
      "     Neutral       0.54      0.51      0.52      1233\n",
      "\n",
      "    accuracy                           0.56      7178\n",
      "   macro avg       0.52      0.57      0.53      7178\n",
      "weighted avg       0.57      0.56      0.56      7178\n",
      "\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Save the model",
   "id": "ba2dc8b0859dacae"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-21T23:26:43.319493Z",
     "start_time": "2024-12-21T23:26:43.306152Z"
    }
   },
   "cell_type": "code",
   "source": "torch.save(model.state_dict(), model_output_path)",
   "id": "de8e9bda49a036b5",
   "outputs": [],
   "execution_count": 14
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
